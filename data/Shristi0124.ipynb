{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e788fa72-67eb-4654-9063-f99eaedd63e7",
   "metadata": {},
   "source": [
    " Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb1e57-766e-4549-8393-963156c55baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28acd312-58fd-4a32-a65c-041602e19635",
   "metadata": {},
   "source": [
    " Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a6b4f-414e-4a06-b753-018b55065b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual dataset path\n",
    "file_path = 'YAAD/APA-DDoS-Dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preview\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d0ad8-74b7-4834-9235-9bf4d8d7987b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b84b1b3-55c2-4714-9a2d-f002291b581d",
   "metadata": {},
   "source": [
    " Step 3: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecceabb6-9d57-442e-86af-706f3576dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any columns with too many missing values\n",
    "df.dropna(axis=1, thresh=len(df)*0.9, inplace=True)\n",
    "\n",
    "# Drop rows with remaining NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove duplicates if any\n",
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e28c9-127c-47a7-8cde-978b2b00d77f",
   "metadata": {},
   "source": [
    " Step 4: Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1ef5f-bc93-420f-888a-988577112cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with actual label column\n",
    "df['Label'] = df['Label'].apply(lambda x: 1 if 'DDoS' in str(x) else 0)  # 1 = attack, 0 = benign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1f9ca-b5cc-4dd7-97f4-dca9ebeb1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: assume 'df' is your full DataFrame\n",
    "dataplt = df.copy()  # assign it to dataplt\n",
    "\n",
    "# Set style and plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "ppt = sns.pairplot(dataplt, hue=\"Label\", height=3)\n",
    "\n",
    "\n",
    "# Pairplot with less time complexity and bad visibility.\n",
    "sns.set_style(\"whitegrid\");\n",
    "\n",
    "ppt =sns.pairplot(dataplt, hue=\"Label\", height=3);\n",
    "handles = ppt._legend_data.values()\n",
    "labels = dataplt[\"Label\"].unique()\n",
    "legend = plt.legend(handles=handles, labels=labels.all(), loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b28ae-e786-4e82-b49f-1550ce12ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  dataplt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f8e80-5440-47aa-9527-bd3f2d8e81cc",
   "metadata": {},
   "source": [
    "Step 5: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b996c-3b7d-422e-b14f-13dae9352821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric and label columns for features\n",
    "X = df.select_dtypes(include=[np.number]).drop('Label', axis=1)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Labels\n",
    "y = df['Label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3fe61a-7d16-4606-92ee-3e52e7082fb3",
   "metadata": {},
   "source": [
    "Step 6: Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ff28c-3a86-425c-8c51-b81affc1ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Test samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36655306-0aa1-4003-b9dd-4831d3a31e77",
   "metadata": {},
   "source": [
    "Step 7: Train a Baseline Model (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90799969-4a3e-4b29-bb3c-f59d140d7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4591f999-a2d8-445f-8f5f-393ed1077e86",
   "metadata": {},
   "source": [
    "Step 7: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f6b97-452a-4f30-84fd-29789ddd1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff45fc2-f8cf-48b1-bd41-83f1ddd5050f",
   "metadata": {},
   "source": [
    "Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108318a5-09fd-4823-a2b4-67ad378aee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=[\"Benign\", \"DDoS\"], yticklabels=[\"Benign\", \"DDoS\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca57f5e4-d9cf-4c78-8f95-0c143bc01926",
   "metadata": {},
   "source": [
    "Step 4: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fb9c4-25fa-4fcf-a04c-ca4d76e1c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Show top 10 features\n",
    "top_features = [X.columns[i] for i in indices[:10]]\n",
    "print(\"Top 10 important features:\", top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac17d12-2224-4482-9695-cb49cd1f2e84",
   "metadata": {},
   "source": [
    "Step 1: Hyperparameter Tuning (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4f663-8e04-446e-a4e4-c8242201857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be885e3d-086e-43a3-9aa6-1246ac8653e4",
   "metadata": {},
   "source": [
    "Step 2: Model Evaluation (on Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd9ed8-f351-4edd-aa44-5aded4c4b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_prob))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe4386-96f5-484e-824f-6357bfef042d",
   "metadata": {},
   "source": [
    " Step 4: Model Deployment Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20c398-b406-4175-a0dd-767872601a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(best_model, 'ddos_detector_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7fc63b-9fd2-4d34-b28b-14cb024d14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this as app.py\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = joblib.load('ddos_detector_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json['data']\n",
    "    data_scaled = scaler.transform([data])\n",
    "    prediction = model.predict(data_scaled)[0]\n",
    "    return jsonify({'prediction': int(prediction)})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d3e96-c4c7-41f6-bb4a-14f8aabf1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"data\": [0.1, 0.23, 4.5, 23, 0, ...]  # scaled features vector (same order as training)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62caac7d-a473-4114-af02-ef12c7502b9a",
   "metadata": {},
   "source": [
    " Step 1: Setup Real-Time Traffic Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024cb29-b77b-43c2-940d-0777e2b0c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import sniff\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load model and scaler\n",
    "model = joblib.load('ddos_detector_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Feature extraction function (very basic example)\n",
    "def extract_features(pkt):\n",
    "    try:\n",
    "        return [\n",
    "            len(pkt),  # packet size\n",
    "            pkt.ttl if hasattr(pkt, 'ttl') else 64,  # Time to live\n",
    "            pkt.time  # Timestamp\n",
    "        ]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Callback for each packet\n",
    "def process_packet(pkt):\n",
    "    features = extract_features(pkt)\n",
    "    if features:\n",
    "        data_scaled = scaler.transform([features])\n",
    "        prediction = model.predict(data_scaled)[0]\n",
    "        if prediction == 1:\n",
    "            print(\"ðŸš¨ DDoS Attack Detected\")\n",
    "        else:\n",
    "            print(\"âœ… Normal Traffic\")\n",
    "\n",
    "# Start live capture\n",
    "sniff(filter=\"ip\", prn=process_packet, store=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d941d3-9599-4014-b9ff-b65aa5a6e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f76901a-7cf1-49b9-ab07-6105fd51786c",
   "metadata": {},
   "source": [
    " Step 2: Integration into Network Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb540bf6-f212-4fb8-b61f-74964af7d996",
   "metadata": {},
   "source": [
    " Step 3: Live Testing & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69dd81-65cd-48a9-ab8e-994d2ccca7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (your-env-name)",
   "language": "python",
   "name": "your-env-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
